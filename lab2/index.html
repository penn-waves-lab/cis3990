<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Title -->
    <title>CIS3990 Lab 2</title>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
        integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
    <!-- Custom styles for this template -->
    <link href="css/styles.css" rel="stylesheet">
    <link href="css/scrolling-nav.css" rel="stylesheet">
    <!-- For math function -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
      
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
</head>


<body id="main">
    <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-penn-blue">
        <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="../index.html">CIS3990</a>
            <a class="navbar-brand js-scroll-trigger" href="index.html#main">Home</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggle">
                <ul class="navbar-nav ml-auto">
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="index.html#installation">Installation</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="index.html#sec1">Section 1</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="index.html#sec2">Section 2</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="index.html#sec3">Section 3</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="index.html#submission">Submission Instructions</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container" style="padding-top: 20px; padding-bottom: 20px">
        <div class="section">
            <p />
            <h1 style="font-size: 32px">Lab 2</h1>
            <p />
            <p>The goal of this lab is to implement code in Jupyter Notebook to detect human gestures and
                monitor breathing via sound signals. We have provided you with all of the code to show
                the processed outcome (eg spectrograms, plots etc.), but you will need to implement some
                parts of the FMCW code to enable the sensing.</p>
            <p>The human gestures part of this lab is based on an MIT 6.808 project in Spring 2021 by Cooper Jones,
                Willie Zhu and Jan Wojcik.</p>
            <p>Start by downloading the <a href="code/FMCW.zip">Jupyter notebook</a> for this lab.</p>
            <p> <strong>Known problem: </strong>Do not use Google Colab to run the .ipynb. The PortAudio library cannot
                be used in Colab.</p>
        </div>
    </div>

    <div class="container">
        <h5 id="installation">Installation</h5>
        <p>Start by <a href="https://www.anaconda.com/">downloading the Anaconda software</a>. After installing, open
            the Anaconda Navigator and click on launch for Jupyter Notebook. It should open in Chrome.

        <p>In the browser, change directory to the folder where the provided notebook locates, and then click
            on the notebook. It should open a new tab. You can see the notebook and the code.</p>
        <p> For those unfamiliar with .ipynb, the order in which you run the cells matter, so be aware when going back
            through different parts of this notebook (as in section 2)</p>
        <p>Read this <a href="https://en.wikipedia.org/wiki/Continuous-wave_radar">link</a> before you begin lab2.
            Also, review <a
                href="https://penn-waves-lab.github.io/cis3990-24spring/slides/lec4-device-free-localization.pdf">lec4
                slides</a>
            if needed. Make sure you understand the following questions</p>
        <ul>
            <li>What is FMCW? How does it work to detect distance?</li>
            <li>Why can we use the phase to track small movement? </li>
        </ul>
    </div>
    <br>

    <div class="container">
        <h5 id="sec1">Section 1 ‚Äî Recording and processing FMCW Signals</h5>
        <p>The goal of this section is to successfully obtain spectrograms from the recorded FMCW signals. In the next
            section you will extract gestures using these spectrograms.</p>
        <p>The basic sequence is as follows:</p>
        <ol>
            <li>Simultaneously transmit and receive FMCW (i.e, chirp) signals from your laptop</li>
            <li> Multiply the received FMCW chirp with the transmitted FMCW chirp, for each chirp you should see a peak
                in the baseband</li>
            <li>Pass the output of the multiplication through a low pass filter</li>
            <li>Extract the peak and convert it to distance using the slope of the FMCW chirp and the speed of sound
            </li>
            <li>Record distance estimates from different chirps and plot them as a function of time</li>
            <li>Observe how the plot changes with different gestures.</li>
        </ol>
    </div>
    <br>

    <div class="container">
        <h5 id="sec1.1">Section 1.1: Transmit and receive a single tone</h5>
        <p>Before you can start transmitting and receiving FMCW chirp signals, you need to know how to transmit a single
            frequency, what it sounds like, what it looks like in the time and frequency domains, and what its
            spectrogram looks like.</p>
        <p>To do this you will need to implement the ‚Äúplay_and_record‚Äù function. Given a frequency, sampling rate, and
            duration as input, this function should play the corresponding sound (using the speaker) and record it
            (using the microphone); it should also return the recorded sound.</p>
        <p>You should write code to do the following:</p>
        <ol>
            <li>Create the time domain signal x of frequency. It is a single tone (or frequency), i.e., a single sine or
                cosine wave.</li>
            <li>Play that sound and record it. You can use "y = sd.playrec(x)" to play the signal x, which will be
                recorded by the microphone and stored in array y.</li>
        </ol>
        <p>Once you have successfully implemented this function, execute the code that comes after the function
            definition. You should hear a loud 10kHz tone for 2 seconds. When you run the following code blocks, you
            should be able to see what the signal looks like in time and frequency and what the spectrogram of the
            signal looks like. </p>
        <p><strong>Known issue for Mac Laptops: </strong> When running the ‚Äúsd.playrec()‚Äù command, Google Chrome might
            ask for permission to get access to the microphone, please give access when you get this message otherwise,
            this command will only play the sound but it will not record it. If you didn't give access then you will
            need to go to microphone settings in Mac and there you can give microphone access to Chrome.</p>
        <p>Once everything is working fine, you should run subsequent blocks to see the received signal in the time
            domain, frequency domain, and the spectrogram.</p>
    </div>
    <br>

    <div class="container">
        <h5 id="sec1.2">Section 1.2: Transmit and receive a FMCW chirp</h5>
        <p>In this task, you will transmit and record a FMCW chirp signal. To do this you will need to implement the
            "play_and_record_chirp" function. Specifically you need to do the following:
        </p>
        <ol>
            <li>Create a chirp signal using the "chirp(t,f0=x,f1=x,t1=x,method='linear').astype(np.float32)" command
                where t is the time vector for your chirp, f0 and f1 are the starting and ending frequencies of your
                chirp, t1 is the chirp duration, and "method" tells you how the frequency should change as a function of
                time.</li>
            <li>Create a variable which has multiple repetitions of this chirp so that the total duration of the signal
                matches ‚Äútotal_duration‚Äù.</li>
            <li>Play that sound and record it. You can use "rx = sd.playrec(tx)" to play the signal tx, which will be
                recorded by the microphone and stored in array rx</li>
        </ol>
        <p><strong>Note: </strong>If the microphone is not able to record a good signal, try increasing the amplitude of
            your transmitted signal (tx) by scaling the entire signal by a constant factor i.e. 500 or 100</p>
        <p>After you have successfully implemented this function, run the subsequent block. You should hear a periodic
            sweep from your speakers.</p>
        <p>Subsequent code blocks do the following:</p>
        <ol>
            <li> Segment the transmitted and received FMCW signal into small chunks so that each chunk contains one FMCW
                chirp signal, this segmented data is stored as an array in ‚Äúrx_data ‚Äúand ‚Äútx_data‚Äù</li>
            <li>Plots the FFT (frequency domain representation) of the first segment , i.e ‚Äúrx_data_sample‚Äù</li>
            <li>Mixes the first segment with the transmitted FMCW segment and plots the FFT of the mixed signal i.e.
                ‚Äúmultiplied_fft‚Äù (you should see a peak here below 5000 Hz)</li>
            <li>Filters the mixed signal using a low pass filter, plots the FFT again and obtains the peak location
                (stored in ‚Äúpeak_location‚Äù)</li>
            <li> Repeats the same steps for all segments (‚Äúall_multiplied‚Äù and ‚Äúall_multiplied_ffts‚Äù) and shows how the
                spectrogram looks like for different consecutive segments.</li>
        </ol>
    </div>
    <br>

    <div class="container">
        <h5 id="sec2">Section 2 ‚Äî Extracting gestures from FMCW signals</h5>
        <p>Notice that the spectrograms look relatively similar. In this section, we will see how performing background
            subtraction allows us to track movements in the environment.</p>
    </div>
    <br>

    <div class="container">
        <h5 id="sec2.1">Section 2.1: Implement <code>background_subtract</code></h5>
        <p>In this task you will perform background subtraction from the received FMCW chirp signals. To do this you
            need to implement the background_subtract function that takes in a series of mixed chirp segment FFTs (i.e.
            all_multiplied_ffts) as input. Specifically you need to do the following:</p>
        <ol>
            <li>For all the chirps c1, c2, ...c_n, average them up and get the mean signal.
                This mean signal represents the background. </li>
            <li>Subtract each chirp with the background signal.</li>
        </ol>
        <p>Subsequent code blocks do the following:
        <ol>
            <li> Get the peak locations for all of the segments and obtain the median peak location</li>
            <li>Use the median peak location to zoom in on the subtracted FFTs (output of ‚Äúbackground_subtract()‚Äù) and
                store it in subtracted_filtered</li>
            <li>Plots the spectrogram of the zoomed in subtracted FFT</li>
            <li>Get the peak location in the subtracted FFT segments and plot them (stored in ‚Äúargmaxes‚Äù)</li>
        </ol>
    </div>
    <br>

    <div class="container">
        <h5 id="sec2.2">Section 2.2: Implement <code>idx_to_distance</code></h5>
        <p>In this task you will estimate the distance using the peak location. To do this you need to implement the
            ‚Äúidx_to_distance‚Äù function which takes the peak location as input. Specifically you need to do the
            following:</p>
        <ol>
            <li> Implement the following equation: <p>ùëëùëñùë†ùë°ùëéùëõùëêùëí=(Œîùëì / ùë†ùëôùëúùëùùëí.ùë£)/2</p>
                <p>where "v" is the speed of sound in air, "slope" is the slope of your FM  W chirp (Hz/s) and " Œîùëì "
                    corresponds to the peak location.</p>
            </li>
            <li>Store the result in the ‚Äúdistance‚Äù variable and return it as an output of the function.</li>
        </ol>
        <p><strong> Hint: </strong>The index of the peak is not equal to Œîùëì because it is not in Hz, how can you
            convert it to Hz?</p>
        <p>Once you have successfully implemented this function, and if you run the next code block. It should plot the
            distance variation as a function of time. </p>
    </div>
    <br>

    <div class="container">
        <h5 id="sec2.3">Section 2.3: Extract Gestures</h5>
        <p>In this task, you will record different hand gestures. Put you hand about 25 cm above the speaker of the Mac.
            While the chirp sound is playing, do the following two gestures separately:</p>
        <ol>
            <li>Move your hand up, bring it down, and then move it back up</li>
            <li>Slowly move your hand up and down 4-5 times</li>
        </ol>
        <p>For each case your hand movement should be aligned with the direction of the sound.</p>

        <br><b>Tasks:</b>
        <ul>
            <li>Take a screenshot of the plot titled ‚ÄúAfter background subtraction‚Äù,
                do this for both gestures separately (2 plots)</li>
            <li>Take a screenshot of the distance vs time plot, do
                this for both gestures separately (2 plots)</li>
        </ul>
    </div>
    <br>

    <div class="container">
        <h5 id="sec3">Section 3 ‚Äî Monitor breathing from FMCW signals</h5>
        <p>In this section, we will use the phase to track small movement. For example, we can use
            the phase of FMCW signal to monitor the breathing.
        </p>
    </div>
    <br>

    <div class="container">
        <h5 id="sec3.1">Section 3.1: Extract the Phase</h5>
        <p>We first implement the missing code in the block under Task 3.1.
            You will need to do background subtraction for the mixed signal, compute the amplitude (absolute
            values) of the FFT result, and compute the phase of the FFT result (use <code>np.angle()</code>).
        </p>

        <p>After that, we are ready to monitor the breathing. Before playing the sound, use your hands
            to hold the mac so that its speaker is <b>directly facing your chest</b>. Keep the mac about
            <b>40 cm</b> away from your chest and hold it still.
        </p>
        <img src="images/front.png" alt="The front side of how to hold the mac" style="width:420px; height:260px;"/>
        <img src="images/back.png" alt="The back side of how to hold the mac" style="width:420px; height:260px;"/>

        <br><br><b>Tasks:</b> Do the following breathing patterns separately and save the screenshots:
        <ol>
            <li>Inhale and exhale for two times. Then hold your breath to the end</li>
            <li>Inhale, hold your breath, exhale, and hold your breath to the end</li>
        </ol>

        <p><b>Note:</b> Keeping the mac 40 cm away from the chest should result in <code>range_bin=15</code>.
            If you do not see the expected breathing plot, you can change the "range_bin"
            variable to nearby integers (e.g., plus minus 5) to search for the plot.
        </p>

        <p>We also provide pre-recorded data for this task. If you still didn't see the expected breathing plot
            after trying several times, you can download them (<a href="code/breathing_1_rangebin=18.npz">breathing_1_rangebin=18.npz</a>
            and <a href="code/breathing_2_rangebin=21.npz">breathing_2_rangebin=21.npz</a>) and use the following code
            to load them:
            <pre><code class="python"># load pre-recorded data
recorded_data = np.load('breathing_1_rangebin=18.npz')
tx = recorded_data['tx']
rx = recorded_data['rx']</code></pre>
Rembember to change the variable <code>range_bin</code> to the one indicated in the file name (i.e., 18 for this file).
        </p>
    </div>
    <br>


    <div class="container">
        <h5 id="submission">Submission Instructions</h5>
        <p>Write your answers to the following items in a single PDF file and name it
            <strong>lab2_${pennkey}.pdf</strong>. Zip this pdf and the .ipynb notebook and submite
            it in the gradescope.
        </p>
        <ol type="1">
            <li>Names and Penn emails</li>
            <li>In the plot entitled ‚ÄúFrequency Domain - Recieved FMCW signal‚Äù, why does the output look the way it
                does?</li>
            <li>In the plot entitled ‚ÄúFrequency Domain - Downconverted FMCW signal‚Äù, why is there a single
                lower-frequency peak? What does this peak correspond to?</li>
            <li>Attach screenshots of Section 2.3:
                <ul>
                    <li>The "After background subtraction" spectrogram </li>
                    <li>The distance vs time plot</li>
                </ul>
                Do this for both two gestures (i.e., 4 plots in total)
            </li>
            <li>Attach screenshots of Section 3.1:
                <ul>
                    <li>The "Breath Monitoring" plot</li>
                </ul>
                Do this for both two breathing patterns (i.e., 2 plots in total)
            </li>
            <li>The number of hours you spent on this lab.</li>
        </ol>
        <!-- <p>Bonus questions (Optional for extra credit) </p>
        <ol>
            <li>Why do we need to send a whole sweep (with a time interval of T) to estimate the gesture, why cant we
                just send two frequencies that are spaced by T seconds?</li>
            <li>Why do we need to subtract the first strongest peak from the remaining peaks to get the distance
                variation?</li>
        </ol> -->
    </div>
    <br>


    <div class="container">
        <div class="section">
            <hr>
            <br>
            </footer>
        </div>
    </div>

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
    <script src="js/scrolling-nav.js"></script>
</body>

</html>